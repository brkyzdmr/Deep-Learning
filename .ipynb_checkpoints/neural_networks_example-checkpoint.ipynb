{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{blue}{\\text{Neural Networks}}$\n",
    "***\n",
    "## $\\color{red}{\\text{Introduction to Perceptron}}$\n",
    "\n",
    "![ArtificialandBiologicalNeuron](Images/bioneuronvsartineuron.png)\n",
    "* <u>**Inputs:**</u> Values of the features<br><br>\n",
    "* **Inputs** are multiplied by a **weight**.\n",
    "* Weights initially start off as random.\n",
    "* Then these results are passed to an **activation function**.\n",
    "* For the reason of zero possibility of the input, we are adding a bias term to the $input\\times weight$ calculation.<br><br>\n",
    "<u>**Perceptron model:**</u><br> \n",
    "$ w: weight \\\\ x: inputs \\\\ b: bias$\n",
    "$$\\sum_{i=0}^n {w_ix_i}+b$$\n",
    "\n",
    "<u>**Simple activation function:**</u> <br>\n",
    "$ w: weight \\\\ x: inputs$\n",
    "$$ output =\n",
    "    \\begin{cases}\n",
    "    0 & \\quad \\text{if } \\sum_{j} {w_ix_i} \\leq \\text  threshold \\\\\n",
    "    1 & \\quad \\text{if } \\sum_{j} {w_ix_i} > \\text  threshold\n",
    "    \\end{cases}\n",
    "$$\n",
    "***\n",
    "## $\\color{red}{\\text{Neural Network}}$\n",
    "\n",
    "![NeuralNetwork](Images/neuralnetwork.jpeg)\n",
    "\n",
    "* <u>Input Layers:</u> Real values from the data\n",
    "* <u>Hidden Layers:</u> Layers in between input and output, 3 or more hidden layers is **deep network** \n",
    "* <u>Output Layer:</u> Final estimate of the output\n",
    "* As you go forwards through more layers, the level of abstraction increases.\n",
    "\n",
    "\n",
    "![ActivationFunctions](Images/activationfunctions.png)\n",
    "[For more information](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)\n",
    "\n",
    "<u>**Cost Function:**</u> It measures how far off the value from the expected value\n",
    "$ \\text {y: true value} \\\\ \\text {a: neuron's prediction}$\n",
    "$$ w \\times x + b = z \\\\ \\sigma(z) = a $$\n",
    "\n",
    "<u>**Quadratic Cost:**</u> $$c = \\sum(y - a)^2 / n$$\n",
    "* We can see that larger errors are more prominent due to the squaring.\n",
    "* Unfortunately this calculation can cause a slowdown in our learning speed.\n",
    "\n",
    "<u>**Cross Entropy:**</u> $$c = (-1/n) \\sum(y \\times ln(a) + (1 - y) \\times ln(1 - a))$$\n",
    "* This cost function allows for faster learning.\n",
    "* The larger the difference, the faster the neuron can learn.\n",
    "\n",
    "[For more information](https://medium.com/@lachlanmiller_52885/understanding-and-calculating-the-cost-function-for-linear-regression-39b8a3519fcb)\n",
    "\n",
    "<u>**Gradient Descent:**</u> An optimization algorithm for findinf the minimum of a function.\n",
    "* To find a local minimum, we take steps proportional to the negative of the gradient.\n",
    "\n",
    "![GradientDescent](Images/gradientdescent.png)\n",
    "\n",
    "* Visually we can see what parameter value to choose to minimize our cost.\n",
    "* Finding this minimum is simple for one dimension.\n",
    "* Using gradient descent we can figure out the vest parameters for minimizing our cost. For example, finding the best values for the weights of the neuron inputs.\n",
    "\n",
    "<u>**Back Propagation:**</u> It used to calculate the error contribution of each neuron after a batch of data is processed.\n",
    "* It relies heavily on the chain rule to go back through the network and calculate these errors.\n",
    "* Back-propagation works by calculation the error at the output and then distributes back through the network layers.\n",
    "* It requires a known desired output for each input value(supervised learning).\n",
    "\n",
    "<u>**Tensorflow Playground:**</u> It is an interactive visualization of neural networks.\n",
    "[Tensorflow Playground](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
